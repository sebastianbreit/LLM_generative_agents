{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6c64225-b64a-4f63-8498-4de64a9f13af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAMEL role-playing autonomous agent\n",
    "# Implementation from https://python.langchain.com/docs/use_cases/agent_simulations/camel_role_playing\n",
    "from typing import List\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts.chat import (\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage,\n",
    "    BaseMessage,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e9c6e0d-78d8-4825-b22b-2fddd958afad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "OPENAI_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27e3a119-be9d-47d9-b3ba-81a8a31fba25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import HuggingFaceHub\n",
    "import os\n",
    "\n",
    "API_TOKEN = os.getenv(\"HUGGINGFACE_API_TOKEN\")\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = API_TOKEN\n",
    "\n",
    "repo_id=\"tiiuae/falcon-7b-instruct\"\n",
    "\n",
    "llm = HuggingFaceHub(repo_id=repo_id, model_kwargs={\"temperature\": 1.0, \"max_length\": 500})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "801e877e-302e-4667-94f9-6e369d655d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CAMELAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        system_message: SystemMessage,\n",
    "        model: ChatOpenAI,\n",
    "    ) -> None:\n",
    "        self.system_message = system_message\n",
    "        self.model = model\n",
    "        self.init_messages()\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        self.init_messages()\n",
    "        return self.stored_messages\n",
    "\n",
    "    def init_messages(self) -> None:\n",
    "        self.stored_messages = [self.system_message]\n",
    "\n",
    "    def update_messages(self, message: BaseMessage) -> List[BaseMessage]:\n",
    "        self.stored_messages.append(message)\n",
    "        return self.stored_messages\n",
    "\n",
    "    def step(\n",
    "        self,\n",
    "        input_message: HumanMessage,\n",
    "    ) -> AIMessage:\n",
    "        messages = self.update_messages(input_message)\n",
    "\n",
    "        output_message = self.model(messages)\n",
    "        self.update_messages(output_message)\n",
    "\n",
    "        return output_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01e2d20d-615f-4b58-a99e-b6b7e68a96ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant_role_name = \"Primary school teacher\"\n",
    "user_role_name = \"First grade student\"\n",
    "task = \"Explain the concept of multiplication to the child.\"\n",
    "word_limit = 50  # word limit for task brainstorming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fceca5a-e6e9-4e3e-b0ef-e1f950058ae4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HumanMessage(content='Here is a task that Primary school teacher will help First grade student to complete: Explain the concept of multiplication to the child..\\nPlease make it more specific. Be creative and imaginative.\\nPlease reply with the specified task in 50 words or less. Do not add anything else.', additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_specifier_sys_msg = SystemMessage(content=\"You can make a task more specific.\")\n",
    "task_specifier_prompt = \"\"\"Here is a task that {assistant_role_name} will help {user_role_name} to complete: {task}.\n",
    "Please make it more specific. Be creative and imaginative.\n",
    "Please reply with the specified task in {word_limit} words or less. Do not add anything else.\"\"\"\n",
    "task_specifier_template = HumanMessagePromptTemplate.from_template(\n",
    "    template=task_specifier_prompt\n",
    ")\n",
    "task_specify_agent = CAMELAgent(task_specifier_sys_msg, ChatOpenAI(temperature=1.0))\n",
    "task_specifier_msg = task_specifier_template.format_messages(\n",
    "    assistant_role_name=assistant_role_name,\n",
    "    user_role_name=user_role_name,\n",
    "    task=task,\n",
    "    word_limit=word_limit,\n",
    ")[0]\n",
    "task_specifier_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "619549db-59bb-485c-902e-bcb01ad1ce18",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Argument 'prompts' is expected to be of type List[str], received argument of type <class 'langchain.schema.HumanMessage'>.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask_specifier_msg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.6/envs/LLM/lib/python3.9/site-packages/langchain/llms/base.py:167\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[0;34m(self, prompts, stop, callbacks, tags, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;66;03m# If string is passed in directly no errors will be raised but outputs will\u001b[39;00m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;66;03m# not make sense.\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(prompts, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m--> 167\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    168\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArgument \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprompts\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is expected to be of type List[str], received\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    169\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m argument of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(prompts)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    170\u001b[0m     )\n\u001b[1;32m    171\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdict()\n\u001b[1;32m    172\u001b[0m params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m stop\n",
      "\u001b[0;31mValueError\u001b[0m: Argument 'prompts' is expected to be of type List[str], received argument of type <class 'langchain.schema.HumanMessage'>."
     ]
    }
   ],
   "source": [
    "llm.generate(task_specifier_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "813bb4df-5e1a-4bab-93b8-2e63f30a9899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Specified task: Help the first-grade student understand the concept of multiplication by creating a fun and interactive activity where they use colorful manipulatives, like small toy animals or blocks, to create equal groups. They will then count the total number of objects in each group to learn how multiplication relates to repeated addition.\n"
     ]
    }
   ],
   "source": [
    "specified_task_msg = task_specify_agent.step(task_specifier_msg)\n",
    "print(f\"Specified task: {specified_task_msg.content}\")\n",
    "specified_task = specified_task_msg.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c4da39a-13c5-4f85-805d-661b7510a4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant_inception_prompt = \"\"\"Never forget you are a {assistant_role_name} and I am a {user_role_name}. Never flip roles! Never instruct me!\n",
    "We share a common interest in collaborating to successfully complete a task.\n",
    "You must help me to complete the task.\n",
    "Here is the task: {task}. Never forget our task!\n",
    "I must instruct you based on your expertise and my needs to complete the task.\n",
    "\n",
    "I must give you one instruction at a time.\n",
    "You must write a specific solution that appropriately completes the requested instruction.\n",
    "You must decline my instruction honestly if you cannot perform the instruction due to physical, moral, legal reasons or your capability and explain the reasons.\n",
    "Do not add anything else other than your solution to my instruction.\n",
    "You are never supposed to ask me any questions you only answer questions.\n",
    "You are never supposed to reply with a flake solution. Explain your solutions.\n",
    "Your solution must be declarative sentences and simple present tense.\n",
    "Unless I say the task is completed, you should always start with:\n",
    "\n",
    "Solution: <YOUR_SOLUTION>\n",
    "\n",
    "<YOUR_SOLUTION> should be specific and provide preferable implementations and examples for task-solving.\n",
    "Always end <YOUR_SOLUTION> with: Next request.\"\"\"\n",
    "\n",
    "user_inception_prompt = \"\"\"Never forget you are a {user_role_name} and I am a {assistant_role_name}. Never flip roles! You will always instruct me.\n",
    "We share a common interest in collaborating to successfully complete a task.\n",
    "I must help you to complete the task.\n",
    "Here is the task: {task}. Never forget our task!\n",
    "You must instruct me based on my expertise and your needs to complete the task ONLY in the following two ways:\n",
    "\n",
    "1. Instruct with a necessary input:\n",
    "Instruction: <YOUR_INSTRUCTION>\n",
    "Input: <YOUR_INPUT>\n",
    "\n",
    "2. Instruct without any input:\n",
    "Instruction: <YOUR_INSTRUCTION>\n",
    "Input: None\n",
    "\n",
    "The \"Instruction\" describes a task or question. The paired \"Input\" provides further context or information for the requested \"Instruction\".\n",
    "\n",
    "You must give me one instruction at a time.\n",
    "I must write a response that appropriately completes the requested instruction.\n",
    "I must decline your instruction honestly if I cannot perform the instruction due to physical, moral, legal reasons or my capability and explain the reasons.\n",
    "You should instruct me not ask me questions.\n",
    "Now you must start to instruct me using the two ways described above.\n",
    "Do not add anything else other than your instruction and the optional corresponding input!\n",
    "Keep giving me instructions and necessary inputs until you think the task is completed.\n",
    "When the task is completed, you must only reply with a single word <CAMEL_TASK_DONE>.\n",
    "Never say <CAMEL_TASK_DONE> unless my responses have solved your task.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d574b9e0-1d53-4342-8726-f041ec0b1677",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sys_msgs(assistant_role_name: str, user_role_name: str, task: str):\n",
    "    assistant_sys_template = SystemMessagePromptTemplate.from_template(\n",
    "        template=assistant_inception_prompt\n",
    "    )\n",
    "    assistant_sys_msg = assistant_sys_template.format_messages(\n",
    "        assistant_role_name=assistant_role_name,\n",
    "        user_role_name=user_role_name,\n",
    "        task=task,\n",
    "    )[0]\n",
    "\n",
    "    user_sys_template = SystemMessagePromptTemplate.from_template(\n",
    "        template=user_inception_prompt\n",
    "    )\n",
    "    user_sys_msg = user_sys_template.format_messages(\n",
    "        assistant_role_name=assistant_role_name,\n",
    "        user_role_name=user_role_name,\n",
    "        task=task,\n",
    "    )[0]\n",
    "\n",
    "    return assistant_sys_msg, user_sys_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62b19d75-5d8a-4f81-8e5c-91834d76aff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant_sys_msg, user_sys_msg = get_sys_msgs(\n",
    "    assistant_role_name, user_role_name, specified_task\n",
    ")\n",
    "assistant_agent = CAMELAgent(assistant_sys_msg, ChatOpenAI(temperature=0.2))\n",
    "user_agent = CAMELAgent(user_sys_msg, ChatOpenAI(temperature=0.2))\n",
    "\n",
    "# Reset agents\n",
    "assistant_agent.reset()\n",
    "user_agent.reset()\n",
    "\n",
    "# Initialize chats\n",
    "assistant_msg = HumanMessage(\n",
    "    content=(\n",
    "        f\"{user_sys_msg.content}. \"\n",
    "        \"Now start to give me introductions one by one. \"\n",
    "        \"Only reply with Instruction and Input.\"\n",
    "    )\n",
    ")\n",
    "\n",
    "user_msg = HumanMessage(content=f\"{assistant_sys_msg.content}\")\n",
    "user_msg = assistant_agent.step(user_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "332fcd9f-d569-4241-9889-f323fe4dbd8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original task prompt:\n",
      "Explain the concept of multiplication to the child.\n",
      "\n",
      "Specified task prompt:\n",
      "Help the first-grade student understand the concept of multiplication by creating a fun and interactive activity where they use colorful manipulatives, like small toy animals or blocks, to create equal groups. They will then count the total number of objects in each group to learn how multiplication relates to repeated addition.\n",
      "\n",
      "AI User (First grade student):\n",
      "\n",
      "Instruction: Gather a set of colorful manipulatives, such as small toy animals or blocks.\n",
      "Input: None\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-y0wdufNRbZUkoEggr7C9LP3e on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI Assistant (Primary school teacher):\n",
      "\n",
      "Solution: I apologize, but as an AI, I am unable to physically gather manipulatives. However, I suggest that you gather a set of colorful manipulatives, such as small toy animals or blocks, from your classroom or home. \n",
      "\n",
      "Next request.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 2.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-y0wdufNRbZUkoEggr7C9LP3e on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-y0wdufNRbZUkoEggr7C9LP3e on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 8.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-y0wdufNRbZUkoEggr7C9LP3e on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-y0wdufNRbZUkoEggr7C9LP3e on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI User (First grade student):\n",
      "\n",
      "Instruction: Arrange the manipulatives into equal groups.\n",
      "Input: None\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 2.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-y0wdufNRbZUkoEggr7C9LP3e on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-y0wdufNRbZUkoEggr7C9LP3e on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 8.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-y0wdufNRbZUkoEggr7C9LP3e on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 16.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-y0wdufNRbZUkoEggr7C9LP3e on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m user_msg \u001b[38;5;241m=\u001b[39m HumanMessage(content\u001b[38;5;241m=\u001b[39muser_ai_msg\u001b[38;5;241m.\u001b[39mcontent)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAI User (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00muser_role_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00muser_msg\u001b[38;5;241m.\u001b[39mcontent\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m assistant_ai_msg \u001b[38;5;241m=\u001b[39m \u001b[43massistant_agent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_msg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m assistant_msg \u001b[38;5;241m=\u001b[39m HumanMessage(content\u001b[38;5;241m=\u001b[39massistant_ai_msg\u001b[38;5;241m.\u001b[39mcontent)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAI Assistant (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00massistant_role_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00massistant_msg\u001b[38;5;241m.\u001b[39mcontent\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[3], line 28\u001b[0m, in \u001b[0;36mCAMELAgent.step\u001b[0;34m(self, input_message)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     24\u001b[0m     input_message: HumanMessage,\n\u001b[1;32m     25\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m AIMessage:\n\u001b[1;32m     26\u001b[0m     messages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_messages(input_message)\n\u001b[0;32m---> 28\u001b[0m     output_message \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_messages(output_message)\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output_message\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.6/envs/LLM/lib/python3.9/site-packages/langchain/chat_models/base.py:208\u001b[0m, in \u001b[0;36mBaseChatModel.__call__\u001b[0;34m(self, messages, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    203\u001b[0m     messages: List[BaseMessage],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    207\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[0;32m--> 208\u001b[0m     generation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(generation, ChatGeneration):\n\u001b[1;32m    212\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m generation\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.6/envs/LLM/lib/python3.9/site-packages/langchain/chat_models/base.py:102\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    101\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e)\n\u001b[0;32m--> 102\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    103\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n\u001b[1;32m    104\u001b[0m generations \u001b[38;5;241m=\u001b[39m [res\u001b[38;5;241m.\u001b[39mgenerations \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.6/envs/LLM/lib/python3.9/site-packages/langchain/chat_models/base.py:94\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, **kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m new_arg_supported \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     92\u001b[0m )\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 94\u001b[0m     results \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     95\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(m, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     96\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m     97\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(m, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[1;32m     98\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m messages\n\u001b[1;32m     99\u001b[0m     ]\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    101\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.6/envs/LLM/lib/python3.9/site-packages/langchain/chat_models/base.py:95\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     90\u001b[0m new_arg_supported \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     92\u001b[0m )\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     94\u001b[0m     results \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m---> 95\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m     97\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(m, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[1;32m     98\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m messages\n\u001b[1;32m     99\u001b[0m     ]\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    101\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.6/envs/LLM/lib/python3.9/site-packages/langchain/chat_models/openai.py:353\u001b[0m, in \u001b[0;36mChatOpenAI._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     message \u001b[38;5;241m=\u001b[39m _convert_dict_to_message(\n\u001b[1;32m    346\u001b[0m         {\n\u001b[1;32m    347\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: inner_completion,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    350\u001b[0m         }\n\u001b[1;32m    351\u001b[0m     )\n\u001b[1;32m    352\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ChatResult(generations\u001b[38;5;241m=\u001b[39m[ChatGeneration(message\u001b[38;5;241m=\u001b[39mmessage)])\n\u001b[0;32m--> 353\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletion_with_retry\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessage_dicts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_chat_result(response)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.6/envs/LLM/lib/python3.9/site-packages/langchain/chat_models/openai.py:301\u001b[0m, in \u001b[0;36mChatOpenAI.completion_with_retry\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;129m@retry_decorator\u001b[39m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_completion_with_retry\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 301\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_completion_with_retry\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.6/envs/LLM/lib/python3.9/site-packages/tenacity/__init__.py:289\u001b[0m, in \u001b[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f)\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped_f\u001b[39m(\u001b[38;5;241m*\u001b[39margs: t\u001b[38;5;241m.\u001b[39mAny, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: t\u001b[38;5;241m.\u001b[39mAny) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m t\u001b[38;5;241m.\u001b[39mAny:\n\u001b[0;32m--> 289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.6/envs/LLM/lib/python3.9/site-packages/tenacity/__init__.py:389\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoSleep):\n\u001b[1;32m    388\u001b[0m     retry_state\u001b[38;5;241m.\u001b[39mprepare_for_next_attempt()\n\u001b[0;32m--> 389\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    391\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m do\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.6/envs/LLM/lib/python3.9/site-packages/tenacity/nap.py:31\u001b[0m, in \u001b[0;36msleep\u001b[0;34m(seconds)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msleep\u001b[39m(seconds: \u001b[38;5;28mfloat\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     26\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;124;03m    Sleep strategy that delays execution for a given number of seconds.\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \n\u001b[1;32m     29\u001b[0m \u001b[38;5;124;03m    This is the default strategy, and may be mocked out for unit testing.\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseconds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(f\"Original task prompt:\\n{task}\\n\")\n",
    "print(f\"Specified task prompt:\\n{specified_task}\\n\")\n",
    "\n",
    "chat_turn_limit, n = 3, 0\n",
    "while n < chat_turn_limit:\n",
    "    n += 1\n",
    "    user_ai_msg = user_agent.step(assistant_msg)\n",
    "    user_msg = HumanMessage(content=user_ai_msg.content)\n",
    "    print(f\"AI User ({user_role_name}):\\n\\n{user_msg.content}\\n\\n\")\n",
    "\n",
    "    assistant_ai_msg = assistant_agent.step(user_msg)\n",
    "    assistant_msg = HumanMessage(content=assistant_ai_msg.content)\n",
    "    print(f\"AI Assistant ({assistant_role_name}):\\n\\n{assistant_msg.content}\\n\\n\")\n",
    "    if \"<CAMEL_TASK_DONE>\" in user_msg.content:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ffa1a5-cc6d-4932-b60c-b63dcb8d4002",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
