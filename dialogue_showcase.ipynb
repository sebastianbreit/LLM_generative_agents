{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb725cd5-11a0-4e5d-913f-69f77e259ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlamaTokenizer, LlamaForCausalLM, AutoTokenizer, AutoModelForCausalLM\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from memory import Memory\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54cc5f5-2132-41ad-86f0-65dab3149e07",
   "metadata": {},
   "source": [
    "## Check if Cuda is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6327c8db-a01a-4edb-a63d-b655b284f76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a166fc7-6bfe-4e85-91f9-22f3dfcb9950",
   "metadata": {},
   "source": [
    "## Load Dialogue Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8839639-3e96-4e22-83e0-7399dd8c31d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"databricks/dolly-v2-3b\", padding_side=\"left\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"databricks/dolly-v2-3b\", device_map=\"auto\",\n",
    "                                             torch_dtype=torch.bfloat16, cache_dir='dolly-v2-3b')\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69c3b52-f7d0-4212-afcb-41e22a2fc665",
   "metadata": {},
   "source": [
    "## Sentence Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe065b7-eb0c-4936-bd78-27f68c047670",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_model = SentenceTransformer(\n",
    "        'sentence-transformers/paraphrase-MiniLM-L6-v2',\n",
    "        cache_folder='paraphrase-MiniLM-L6-v2'\n",
    "    )\n",
    "print(sentence_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd63b92-96e5-4fa4-8064-c77418a0766b",
   "metadata": {},
   "source": [
    "## Define Player and NPC Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50858677-eda1-467d-95bb-23e5f7d361af",
   "metadata": {},
   "outputs": [],
   "source": [
    "npc_name = 'Balgruuf the Greater'\n",
    "player_name = 'Player'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ccb6da-ee36-4681-9677-2d6426a5c139",
   "metadata": {},
   "source": [
    "## Define Memories with Importance Rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bd4da8-7ee4-4f31-91a4-76255a0966f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "memories = [\n",
    "        'is the jarl of Whiterun.',\n",
    "        'is a Nord'\n",
    "        'resides in the great hall Dragonsreach in Whiterun.',\n",
    "        'wears noble cloths.',\n",
    "        'wears a crown.',\n",
    "        'has a unique war axe.',\n",
    "        'has a brother called Hrongar.',\n",
    "        'has three children called Frothar, Dagny and Nelkir.',\n",
    "        'has no wife.',\n",
    "        'puts Whiteruns interests first.',\n",
    "        'did not permit the Imperials to garrison soldiers in the city.',\n",
    "        'takes no sides in the war.',\n",
    "        'is always on the side of Whiterun',\n",
    "        'does not like Urlfric and Galmar, because they attacked Whiterun.',\n",
    "        'does not like the stormcloaks, because they attacked Whiterun.',\n",
    "        'is concerned about the dragons, because they attacked Whiterun.',\n",
    "        'worships Talos, the god of the Nords.',\n",
    "        'hates the Thalmor.',\n",
    "        'is friends with Irileth, because he fought with her in the war.',\n",
    "    ]\n",
    "memories = [npc_name + \" \" + m + \" \" for m in memories]\n",
    "memory_ratings = [4, 1, 2, 1, 2, 3, 3, 2, 5, 2, 3, 4, 3, 3, 5, 2, 4, 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7558ad27-8023-4c9b-a715-259e8dda0f09",
   "metadata": {},
   "source": [
    "## Initialize Memory for Dialogue "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ee3101-4416-4bb2-8493-e8e7ee8bf386",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = Memory(memories, memory_ratings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315888d0-5af0-4924-ac24-ddaa7446766b",
   "metadata": {},
   "source": [
    "# Start the NPC Dialogue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c4aff6-f18e-47a0-8895-a983b5b11ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of sentences used in initial prompt\n",
    "num_sentences = 4\n",
    "\n",
    "# Start conversation history\n",
    "conversation = \"\"\n",
    "\n",
    "# Start dialogue loop\n",
    "while True:\n",
    "    # Get user input\n",
    "    user_input = input('Player: ')\n",
    "    #print(\"\\nPlayer: \" + user_input)\n",
    "\n",
    "    # Generate initial prompt based on current user input\n",
    "    initial_prompt = memory.generate_prompt(user_input, sentence_model, num_sentences=num_sentences)\n",
    "    print(f'\\nInitial Prompt: {initial_prompt}')\n",
    "\n",
    "    # Add user input to conversation history\n",
    "    conversation += f\"\\n{player_name}:\\n\" + user_input + f\"\\n{npc_name}: \"\n",
    "\n",
    "    # Add initial prompt to conversation history to generate model input\n",
    "    input_text = initial_prompt + \"\\n\" + conversation\n",
    "    input_length = len(input_text)\n",
    "\n",
    "    # Tokenize input\n",
    "    input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to('cuda')\n",
    "\n",
    "    # Generate respones from dialogue model\n",
    "    generation_output = model.generate(\n",
    "        input_ids=input_ids, max_new_tokens=64, temperature=0.2\n",
    "    )\n",
    "\n",
    "    # Decode response\n",
    "    output = tokenizer.decode(generation_output[0])\n",
    "\n",
    "    # Limit reponse to NPC response\n",
    "    split_string = output[input_length:].split('\\n', 2) # .split(f'\\n{player_name}:', 1)\n",
    "    response = split_string[1] \n",
    "    print(f\"\\n{npc_name}: \" + response)\n",
    "\n",
    "    # Add response to conversation history\n",
    "    conversation += response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f265765-c2e3-4c28-a80c-9bc49300f5da",
   "metadata": {},
   "source": [
    "## Setting a higher Temperature:\n",
    "This causes more interesting and more creative responses. However, it also causes more hallucinations. This means the dialogue model will more likely make up stuff that is not part of the NPC-memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f07231-7edc-4f02-95fa-28b2637f4dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of sentences used in initial prompt\n",
    "num_sentences = 4\n",
    "\n",
    "# Start conversation history\n",
    "conversation = \"\"\n",
    "\n",
    "# Start dialogue loop\n",
    "while True:\n",
    "    # Get user input\n",
    "    user_input = input('Player: ')\n",
    "    #print(\"\\nPlayer: \" + user_input)\n",
    "\n",
    "    # Generate initial prompt based on current user input\n",
    "    initial_prompt = memory.generate_prompt(user_input, sentence_model, num_sentences=num_sentences)\n",
    "    print(f'\\nInitial Prompt: {initial_prompt}')\n",
    "\n",
    "    # Add user input to conversation history\n",
    "    conversation += f\"\\n{player_name}:\\n\" + user_input + f\"\\n{npc_name}: \"\n",
    "\n",
    "    # Add initial prompt to conversation history to generate model input\n",
    "    input_text = initial_prompt + \"\\n\" + conversation\n",
    "    input_length = len(input_text)\n",
    "\n",
    "    # Tokenize input\n",
    "    input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to('cuda')\n",
    "\n",
    "    # Generate respones from dialogue model\n",
    "    generation_output = model.generate(\n",
    "        input_ids=input_ids, max_new_tokens=64, temperature=0.8\n",
    "    )\n",
    "\n",
    "    # Decode response\n",
    "    output = tokenizer.decode(generation_output[0])\n",
    "\n",
    "    # Limit reponse to NPC answer\n",
    "    split_string = output[input_length:].split('\\n', 2) # .split(f'\\n{player_name}:', 1)\n",
    "    response = split_string[1] \n",
    "    print(f\"\\n{npc_name}: \" + response)\n",
    "\n",
    "    # Add response to conversation history\n",
    "    conversation += response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f6d946-0673-4edb-bb0a-9ccfcac74114",
   "metadata": {},
   "source": [
    "## Adding more sentences to the inital prompt\n",
    "This causes the initial prompt to contain more information. However, the quality of the inital prompt is more dependent on the quality of the similarity scoring. Therefore a longer input prompt is not always usefull."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c019f1a-5af9-4709-9953-1d57accdb2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of sentences used in initial prompt\n",
    "num_sentences = 4\n",
    "\n",
    "# Start conversation history\n",
    "conversation = \"\"\n",
    "\n",
    "# Start dialogue loop\n",
    "while True:\n",
    "    # Get user input\n",
    "    user_input = input('Player: ')\n",
    "    #print(\"\\nPlayer: \" + user_input)\n",
    "\n",
    "    # Generate initial prompt based on current user input\n",
    "    initial_prompt = memory.generate_prompt(user_input, sentence_model, num_sentences=num_sentences)\n",
    "    print(f'\\nInitial Prompt: {initial_prompt}')\n",
    "\n",
    "    # Add user input to conversation history\n",
    "    conversation += f\"\\n{player_name}:\\n\" + user_input + f\"\\n{npc_name}: \"\n",
    "\n",
    "    # Add initial prompt to conversation history to generate model input\n",
    "    input_text = initial_prompt + \"\\n\" + conversation\n",
    "    input_length = len(input_text)\n",
    "\n",
    "    # Tokenize input\n",
    "    input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to('cuda')\n",
    "\n",
    "    # Generate respones from dialogue model\n",
    "    generation_output = model.generate(\n",
    "        input_ids=input_ids, max_new_tokens=64, temperature=0.2\n",
    "    )\n",
    "\n",
    "    # Decode response\n",
    "    output = tokenizer.decode(generation_output[0])\n",
    "\n",
    "    # Limit reponse to NPC response\n",
    "    split_string = output[input_length:].split('\\n', 2) # .split(f'\\n{player_name}:', 1)\n",
    "    response = split_string[1] \n",
    "    print(f\"\\n{npc_name}: \" + response)\n",
    "\n",
    "    # Add response to conversation history\n",
    "    conversation += response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26d2c02-22ab-4b5a-a2c7-858f622e67f9",
   "metadata": {},
   "source": [
    "## Reducing the number of sentences in the initial prompt\n",
    "This causes worse responses, since the dialogue model heavily relies on the quality of the sentence embedding model. If the sentence embedding model fails, the initial prompt will be unrelated to the players input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5a6af9-bbd8-45f2-b5ac-1cf5389f8c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of sentences used in initial prompt\n",
    "num_sentences = 1\n",
    "\n",
    "# Start conversation history\n",
    "conversation = \"\"\n",
    "\n",
    "# Start dialogue loop\n",
    "while True:\n",
    "    # Get user input\n",
    "    user_input = input('Player: ')\n",
    "    #print(\"\\nPlayer: \" + user_input)\n",
    "\n",
    "    # Generate initial prompt based on current user input\n",
    "    initial_prompt = memory.generate_prompt(user_input, sentence_model, num_sentences=num_sentences)\n",
    "    print(f'\\nInitial Prompt: {initial_prompt}')\n",
    "\n",
    "    # Add user input to conversation history\n",
    "    conversation += f\"\\n{player_name}:\\n\" + user_input + f\"\\n{npc_name}: \"\n",
    "\n",
    "    # Add initial prompt to conversation history to generate model input\n",
    "    input_text = initial_prompt + \"\\n\" + conversation\n",
    "    input_length = len(input_text)\n",
    "\n",
    "    # Tokenize input\n",
    "    input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to('cuda')\n",
    "\n",
    "    # Generate respones from dialogue model\n",
    "    generation_output = model.generate(\n",
    "        input_ids=input_ids, max_new_tokens=64, temperature=0.2\n",
    "    )\n",
    "\n",
    "    # Decode response\n",
    "    output = tokenizer.decode(generation_output[0])\n",
    "\n",
    "    # Limit reponse to NPC response\n",
    "    split_string = output[input_length:].split('\\n', 2) # .split(f'\\n{player_name}:', 1)\n",
    "    response = split_string[1] \n",
    "    print(f\"\\n{npc_name}: \" + response)\n",
    "\n",
    "    # Add response to conversation history\n",
    "    conversation += response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346be7fc-1575-4976-b43e-99a89b57f805",
   "metadata": {},
   "source": [
    "## Dont use memory weighting\n",
    "The scores of each memory are independent of the memory ratings. Therefore the dialogue models reponses are closer to the player input. However, the dialogue model neglects important parts of the NPCs memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ff5729-5bdf-4579-b991-0946fd1b6634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of sentences used in initial prompt\n",
    "num_sentences = 4\n",
    "\n",
    "# Start conversation history\n",
    "conversation = \"\"\n",
    "\n",
    "# Start dialogue loop\n",
    "while True:\n",
    "    # Get user input\n",
    "    user_input = input('Player: ')\n",
    "    #print(\"\\nPlayer: \" + user_input)\n",
    "\n",
    "    # Generate initial prompt based on current user input\n",
    "    initial_prompt = memory.generate_prompt(user_input, sentence_model, num_sentences=num_sentences, weighted=False)\n",
    "    print(f'\\nInitial Prompt: {initial_prompt}')\n",
    "\n",
    "    # Add user input to conversation history\n",
    "    conversation += f\"\\n{player_name}:\\n\" + user_input + f\"\\n{npc_name}: \"\n",
    "\n",
    "    # Add initial prompt to conversation history to generate model input\n",
    "    input_text = initial_prompt + \"\\n\" + conversation\n",
    "    input_length = len(input_text)\n",
    "\n",
    "    # Tokenize input\n",
    "    input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to('cuda')\n",
    "\n",
    "    # Generate respones from dialogue model\n",
    "    generation_output = model.generate(\n",
    "        input_ids=input_ids, max_new_tokens=64, temperature=0.2\n",
    "    )\n",
    "\n",
    "    # Decode response\n",
    "    output = tokenizer.decode(generation_output[0])\n",
    "\n",
    "    # Limit reponse to NPC response\n",
    "    split_string = output[input_length:].split('\\n', 2) # .split(f'\\n{player_name}:', 1)\n",
    "    response = split_string[1] \n",
    "    print(f\"\\n{npc_name}: \" + response)\n",
    "\n",
    "    # Add response to conversation history\n",
    "    conversation += response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
